# ddia读书笔记
## 检索
### 哈希索引
bitcask使用的默认存储引擎.高读写性能.  
value可以及时获取(key只需要一次hash就能找到位置)  
key放入内存,value可以超过内存大小,放在磁盘.只要一次磁盘寻址,可以把value查找.(已经在内存的v直接返回).磁盘中记录的是**key-value**  
内容存放在磁盘,通过追加日志的形式存储.  
可以理解为磁盘中的数据是不管旧数据,新数据写入的时候直接在原来文件后面追加内容,只要更新哈希表即可  
```ditaa
┌───────────────────memory─────────────────────────────┐
│                                                      │
│                                                      │
│ ┌───────────┬────┬────┬────┬────┐                    │
│ │ hash(key) │key1│key2│... │keyN│                    │
│ └───────────┴────┴────┴────┴────┘                    │
│                │    │                                │
│                │    │                                │
└────────────────┼────┼────────────────────────────────┘
                 │    │                                 
                 │    │                                 
                 │    │                                 
┌────────────────┼────┼───────────────disk─────────────┐
│                ▼    ▼                                │
│             ┌────┬────┬────┬────┬────┬────┬────┬────┐│
│             │val1│val2│val3│val1│val1│val2│val3│val1││
│             └────┴────┴────┴────┴────┴────┴────┴────┘│
│                                                      │
│                                                      │
│                                                      │
│                                                      │
│                                                      │
└──────────────────────────────────────────────────────┘
```

但是追加后体积变大,事实上的日志系统是不需要很多重复的旧数据.所以需要日志文件压缩  
![](../imgs/ddia/日志压缩.png)
把旧的数据冲洗掉,保留新的数据  

事实上redis也是使用hash方式存储kv  

找value快,但是不好维护  
#### 需要考虑的问题
- 文件格式
    - 二进制最好,空间小,紧凑.更快更简单
- 删除记录
    - 不是马上删除,是在数据文件中打标.后续合并日志的时候删除
- 崩溃恢复
    - db重启,内存中hashmap会丢失.将每个段hashmap快照存储在磁盘,方便恢复加快
- 部分写入记录
    - 写入的时候db崩溃.通过数据文件校验值,发现损坏并丢弃
- 并发控制
    - 多个线程写入导致数据文件紊乱.实际实现使用一个写线程

#### 覆盖写入日志的好处
对比直接在磁盘中对应位置修改,覆盖写入的好处:
- 追加顺序写,比随机写快
- 数据文件追加,不需要担心重写时候崩溃(如果是覆盖写入,新旧数据混合,崩溃的时候需要多加校验)
- 合并旧段避免文件碎片化

#### 哈希的缺陷
- 必须放内存.对磁盘不友好(key很大的时候,很难做到磁盘的顺序写.而且哈希冲突的处理代价大)
- 区间查询效率低(需要全部key拉出来,不能直接区间查询)

### SSTable LSM-Tree
上面的做法中数据文件的key是没有顺序的.在SSTable中,需要保证数据文件的**Key有序**  
Sorted String Table(SSTable)优点:  
- 合并更高效
    - 类似合并排序算法,拉出多个文件对比每个键(因为键已经有序).输出最小最新的键到文件中  
![](../imgs/ddia/SSTable合并.png)
- 查找方便
    - 不需要在内存知道key的索引位置.根据类似二分的方法,根据key的对比可以确定目标key的区间.从而完成搜索
![](../imgs/ddia/SSTable搜索.png)

#### 构建SSTable
- 写入时候加入到平衡树(红黑树等),这个表成为内存表
- 内存超过阈值(一般几M),作为SSTable文件写入磁盘
    - 因为已经有序,写入快
    - 写入磁盘同时新写入写到新的内存表实例
- 读请求先读内存表,然后最新磁盘,然后次新,如此类推到gg
- 后台周期性执行合并&压缩,丢弃被删除或者被覆盖的值

这种做法存在问题:请求写入成功没有及时写入磁盘的数据在机器宕机之后会丢失.补救措施:使用WAL,写入之后追加到日志.崩溃之后从日志恢复  
```ditaa
                                                               ┌────┐        
                                          ┌────────────────────│cli │        
                                          │                    └────┘        
                                          │                                  
   ┌──────────────────────────────────────┼────────────────────────────────┐ 
   │                                      ├────────────────┐               │ 
   │                               2.write rb_tree         │               │ 
   │                                      ▼                │               │ 
   │             Λ                        Λ                │               │ 
   │            ╱ ╲                      ╱ ╲               │               │ 
   │           ╱   ╲                    ╱   ╲              │               │ 
   │          ╱     ╲                  ╱     ╲             │               │ 
   │         ╱       ╲                ╱       ╲            │               │ 
   │        ╱         ╲              ╱         ╲           │               │ 
   │       ▕           ▏            ▕           ▏      1.write             │ 
   │        ╲         ╱              ╲         ╱         WAL               │ 
   │         ╲       ╱                ╲       ╱            │               │ 
   │          ╲     ╱                  ╲     ╱             │               │ 
   │           ╲   ╱                    ╲   ╱              │               │ 
   │            ╲ ╱                      ╲ ╱               │               │ 
   │             V                        V                │               │ 
   │             │                                         │               │ 
   └─────────────┼─────────────────────────────────────────┼───────────────┘ 
                 │                                         │                 
               save                                        │                 
┌────────────────┼─────────────────────────────────────────┼────────────────┐
│                │                                         │                │
│                ▼                                         │                │
│      ┌───────────────────┐     ┌───────────────────┐     │                │
│      │        Log        │     │        WAL        │◀────┘                │
│      └───────────────────┘     └───────────────────┘                      │
│                                                                           │
│      ┌───────────────────┐                                                │
│      │      Log New      │◀───┐                                           │
│      └───────────────────┘compress                                        │
│                               │                                           │
│                               │                                           │
│    ┌ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ┐                │
│      ┌───────────────────────────────────────────────┐                    │
│    │ │                    Log T1                     │   │                │
│      └───────────────────────────────────────────────┘                    │
│    │ ┌───────────────────────────────────────────────┐   │                │
│      │                    Log T2                     │                    │
│    │ └───────────────────────────────────────────────┘   │                │
│      ┌───────────────────────────────────────────────┐                    │
│    │ │                    Log T3                     │   │                │
│      └───────────────────────────────────────────────┘                    │
│    │                                                     │                │
│     ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─                 │
└───────────────────────────────────────────────────────────────────────────┘
```

### LSM-Tree
SSTable是levelDb使用的  
基于合并和压缩排序文件原理的存储引擎通常叫Log-Structured Merge-Tree,LSM Tree  

Lucene(Elasticsearc的搜索引擎)的倒排映射是key=>doc id,使用累SSTable的排序,后续需要合并  

#### 优化
- LSM-Tree的做法读不友好
    - 如果key不存在,其实可能会遍历整个文件.
    - 所以会使用**布隆过滤器**.返回不存在的可以断定key不存在.但是返回存在的是可能存在,才继续在文件数据中搜索  
- SSTable的压缩和合并顺序实际
    - 分层压缩:LevelDb,RcoksDb.
        - key的范围分裂成多个更小的SSTable,旧数据移动到单独的"层级".压缩可以逐步进行,节省磁盘
    - 大小压缩:HBase
        - 较新和较小的SSTable被连续合并到较旧和较大的SSTable

总体来说,SSTable对写比较友好.因为顺序写

### B树
普遍使用.是覆盖磁盘原来value位置的写.  
将数据库分解成若干大小固定块或者页,页是内部最小读/写单元.接近底层硬件,磁盘尽量可以顺序摆放  
节点值是磁盘地址(页的引用)  
![](../imgs/ddia/B树结构.png)
如果当前节点不够,可能进行分裂  
![](../imgs/ddia/B树分裂.png)

分裂操作比较危险.如果分裂的时候db写入部分页然后崩溃,索引被破坏  
为了能从崩溃中恢复,常见B树实现要有WAL.必须先更新WAL再进行树的修改.  
多线程访问B树需要锁  

#### 优化
- 不使用WAL做崩溃恢复,使用COW,修改页写入不同位置,父的作为新版本被创建
- 保存键的压缩信息,节省页空间
- 为了查询按照顺序扫描,一般设计成磁盘相邻布局.但是树变大之后顺序很难维护.
- 添加额外节点,例如叶子页面有向左向右的节点指向同级的兄弟页,不需要返回到上一级也可以继续顺序扫描
- 分型树

### 对比B树 LSM树
LSM树写快 B树读快(LSM读可能遍历所有数据,B树写要更改树以及维护磁盘顺序)
#### LSM树
优点:  
LSM树的数据压缩占用磁盘io  
但是通常写吞吐更高,也支持更好的压缩(通常SSTable的大小比B树小)  
定期压缩消除碎片,这点B树做不到  

缺点:  
压缩过程干扰读写  
占用磁盘IO.这点B树响应快  
db的数据量越大,压缩需要的磁盘带宽越多  
对比B树key的val对应一个空间,但是LSM树会有多个副本.  
LSM树提供实务能力  

### 其他索引
二级索引,CREATE INDEX创建二级索引,val是主键  
二级索引key不是唯一的,一级索引(主键)key是唯一的  
二级索引如果有多个一样的key,可以通过值成为标识,或者追加其他标识  

#### 在索引中存值
存实际的行或者其他行的引用(二级索引就是存储主键)  
存储其他行引用叫做堆文件
更新值不改key,堆文件高效(只要新的val字节数不大于旧val)  
如果新val字节大于旧val,可能需要移动数据.
- 所有索引要更新新的堆文件位置
- 或者旧的堆位置保留一个间接指针

聚集索引:索引行直接存储到索引中(数据内容存在索引)
表的主键是聚集索引,二级索引引用主键 -->mysql做法  

覆盖索引:非聚集索引中写入数据(只能读,还要更新两次)  

#### 全文索引
leveldb 会对数据使用hash索引  
lucene中是类似字典树的索引机制  

#### 内存 磁盘
保存内存更快不是因为在内存读取快,是因为避免使用写磁盘的格式对内存数据结构的编码开销,以及直接在磁盘很难实现直接在内存能实现的数据结构模型  

### 列式存储
就是不分多个表,一个打标搞定  
对于搜索条件,很多时候数据量多,但是某一列的内容范围有限(例如product_id).使用位图和游程编码可以加快查询  
位图直接根据条件做位操作.游程编码泽根据<有多少个0><有多少个1><有多少个0><...>这样的方式压缩位图.  
![](../imgs/ddia/压缩位图.png)  

## 编码和演化
### thrif 
![](../imgs/ddia/thriftBinary.png)
![](../imgs/ddia/thrifCompact.png)

### protobuf  
![](../imgs/ddia/protobuf.png)
pb有顺序之分,不能随便更改pb顺序.标记号码在解码的时候会反推成对应字段的key  
随便改顺序可能导致数据无效,因为filed_tag确定了位置  
required字段是在解码的时候做检查,不存在报错  
向前兼容:不该老字段  
向后兼容:后面不识别的字段pb解码会跳过  

### 使用消息队列的好处
- 接收方不可用,过载,当缓冲,提高可靠性
- 防止消息发给崩溃进程,防止消息丢失(还是可靠性)
- 避免发送方要感知接收方ip port(这种可以在上层proxy处理)
- 支持广播发送
- 发送方接收方业务解耦

但是使用mq代表了发送方不会理会后续业务流程,真正的异步了  

## 数据复制  
### 同步复制 异步复制
同步复制是主节点复制完成才算写成功.但是这种情况不实际  
一般使用异步模式,未复制到从节点的写请求会丢失.但是应用广泛  
### 配置新从节点
- 主节点数据产生快照
- 快照拷贝到新的从节点
- 从节点连接到主节点请求快照后所有数据变更日志
- 从节点追赶主节点的数据

### 节点失效
#### 从节点失效:追赶式恢复
从主节点获取.如果从节点故障后到上线较长,可能需要拉取快照

#### 主节点失效:切换节点
- 确认主节点失效
    - 心跳包
- 选举新节点
    - 候选节点最好信息和新的对比是最新的
- 重新配置新主几点生效
    - 告诉其他节点
    - 配置原来的slave
    - gg的主节点回来成为从机

切换过程可能出现的问题:
- 使用异步复制,失效前新节点没有原来gg主机的所有数据.gg主机上线,gg节点收到新请求而且没有意识到角色变化,尝试同步其他从节点==>主节点没有复制完成的请求丢弃
- 数据库之外其他数据系统依赖数据库并且协同使用.ex:mysql从节点升为主,同步落后,redis引用了这些主键,导致mysql 和redis不一致. 结果是一些私有数据被泄漏给其他用户->github
- 可能两个节点都认为自己是主节点-->脑裂. 强制关闭某个节点,但是需要设计好,否则可能两个都关闭
- 合适的超时时长(主节点失效)难设置.太短,主节点可能只是因为网络阻塞问题导致节点频繁切换.太长,网络延迟增加.

### 复制日志
#### 基于语句复制
INSERT, UPDATE这些语句直接发送给从节点  
不使用场景:
- 有非确定函数语句.例如NOW RAND这些函数.可以确定执行结果后再同步
- 使用了自增列,所有副本必须按照主机顺序执行,否则可能结果不一致
- 有副作用的语句,例如触发器,函数和存储过程

mysql5.1之前都是基于语句复制.如果有不确定操作,会切换为基于行的复制

#### 基于WAL传输
- 日志结构存储引擎(SSTable)日志是存储方式
- 覆盖写入磁盘的B树,每次预先写入WAL

上面两种情况都可以使用完全一样的日志在节点构建副本,存盘+发给从机  
WAL很底层:那个磁盘块的什么字节发生变化->对版本要求需要一直,否则db可能导入失败

#### 基于行的逻辑日志复制
复制和存储逻辑分离  
关系数据库的逻辑日志:
- 插入,日志有新值
- 删除,日志有唯一标识表示删除行 通常主键
- 更新,有唯一信息标识更新行,新值

事务信息需要另外补充.binlog就是基于行的逻辑日志复制  
这种技术成为变更数据捕获

#### 基于触发器复制
oracle goldengate

### 复制滞后
写主机,读从机.但是可能从机滞后

#### 读自己的写
![](../imgs/ddia/复制滞后.png)
因为从机滞后,读的时候数据和写的不一致.这里读写一致性没有保证到.  
保证读写一致性:
- 用户访问后可能读修改地方,从主机读.否则从从机读.执行需要检测到读的可能性
- 从最快的从机读.要统计从机的同步情况
- cli带时间戳,读的时候看从机的同步情况分发读请求到从机

如果一种情况是:同一个物理用户使用不同的客户端进行读取.这种情况下:
- 确保用户元数据全局共享
- 无法保证路由经过那个数据中心,这种情况只能从主机读

#### 单调读
因为从机备份不同步.同一个cli访问从机A之后访问从机B,如果B未同步A已经同步,可能读取同一个数据先后结果不同
![](../imgs/ddia/单调读.png)
单调读一致性保证这种异常不发生.比强一致性弱,比最终一致性强  
实现方式:确保每个用户总是**从同一个副本读取**.副本失效才到另外的副本读.

#### 前缀一致读
因为从机同步不一致,读从机的时候序列与写入时候的不一致  
![](../imgs/ddia/前缀一致读.png)
前缀一致读避免这种异常,保证按照某个顺序写入,按照某个顺序读.方案是有因果顺序关系(可以理解为时间顺序需要一致)的写入都交给一个分区完成.(例如happened before)

### 多主节点复制
配置多个主节点,可能存在冲突
![](../imgs/ddia/多主节点写冲突.png)
同时对某个数据进行写,不同的主节点都以为写成功,结果同步冲突.  
解决方式:  
- **应用层把同一个写请求固定一个主节点**,但是可能存在一种情况:某个数据中心gg,用户请求要路由到其他数据中心.此时不能确定用户是否能路由到同一台机器(例如用户第一次请求和第二次请求网络入口不同,导致路由到的数据中心不同)
- 使用序列号确保数据一致.不同主节点请求带上不同的序列号,最后只有序列号最大才能覆盖,保证最终一致
    - 序列号可以是时间戳,副本唯一id等.只要能区分并且有规则对比即可

应用层应该自定义冲突解决的逻辑.acid中的c 一致性应该由应用层保证,不是数据库的特性  

### 拓扑结构
![](../imgs/ddia/拓扑结构.png)
环形,星型:某个节点故障,影响其他节点之间的日志复制和转发  
全拓扑:某些网络链路比其他更快,导致复制日志之间覆盖  
![](../imgs/ddia/拓扑导致复制覆盖.png)
可以使用类似版本向量的方法.对于多个主节点m1 m2 m3,<mv1, mv2, mv3>表示多个节点的版本集合,成为版本向量

### 无主节点复制





